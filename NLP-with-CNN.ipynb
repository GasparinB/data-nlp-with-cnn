{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ The goal of this challenge is to show you that we can also use 1D Convolutional Filters to scan sentences üß® (_instead of RNN_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ñ∂Ô∏è Run this cell and make sure the version of üìö [Gensim - Word2Vec](https://radimrehurek.com/gensim/auto_examples/index.html) you are using is ‚â• 4.0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim==4.2.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    "‚ö†Ô∏è **Warning** ‚ö†Ô∏è The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that too many sentences will make your compute slow down, or even freeze - your RAM can overflow. For that reason, **you should start with 10% of the sentences** and see if your computer handles it. Otherwise, rerun with a lower number. \n",
    "\n",
    "‚ö†Ô∏è **DISCLAIMER** ‚ö†Ô∏è **No need to play _who has the biggest_ (RAM) !** The idea is to get to run your models quickly to prototype. Even in real life, it is recommended that you start with a subset of your data to loop and debug quickly. So increase the number only if you are into getting the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 20:03:49.953484: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "### Run this cell to load the data ###\n",
    "######################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that to do NLP, you need to go through one of the following options, as shown here: \n",
    "\n",
    "<img src=\"embedding_or_RNN.png\" width=\"700px\" />\n",
    "\n",
    "But in both cases, you can replace the recurrent layer (top part) by a CNN layer. \n",
    "\n",
    "üëâ We will try both options, starting from the one on the left were the embeddings are learned within the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Concatenate a Keras Embedding with a Conv1Düî• "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a fancy network here. \n",
    "\n",
    "Each of your words is represented by a vector of size N (the size of your embedding). Therefore, as a sentence is a sequence of words, it is represented by a matrix (number of words, N). Consequently, all your sentences are actually represented as matrices once embedded.\n",
    "\n",
    "If you think about it, an image is also a matrix. Said differently, you may represent your sentence of word as a matrix, where each column (or row, depending on how you want to look at it) is a word, and each row (or each column) corresponds to a coordinate in the embedding space. As shown here\n",
    "\n",
    "<img src=\"image_comparison.png\" width=\"500px\" />\n",
    "\n",
    "Well, in that case, as these are close to images, why not using convolutions on them? Yes, convolutions!\n",
    "But, be careful. In the case of images, convolutions are 2-dimensional as the filters scan these pictures left to right and top to bottom. In the case of our sentences, we want the kernel to move _only_ in word by word, hence left to right. It wouldn't make sense to move the kernels top to bottom as _one vector = one word_.\n",
    "\n",
    "So let's create a model that uses convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì You will need to prepare your data. First, tokenize them. Then, you need to pad them (use a value `maxlen` equal to 150). You also might need to compute the size of your vocabulary ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_token,maxlen=150)\n",
    "X_test_pad = pad_sequences(X_test_token,maxlen=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 1D Convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Define a model that has :\n",
    "- an `Embedding` layer: \n",
    "    - `input_dim` = `vocab_size + 1`\n",
    "    - `output_dim` is the embedding space dimension\n",
    "    - `mask_zero` has to be set to `True`. \n",
    "    - Here, for computational reasons, set `input_length` to the maximum length of your observations (that you just defined in the previous question).\n",
    "- a `Conv1D` layer \n",
    "- a `Flatten` layer\n",
    "- a `Dense` layer\n",
    "- an output layer\n",
    "\n",
    "Compile the model accordingly.\n",
    "\n",
    "‚ùó **Remark** ‚ùó The size of the `Conv1D` kernel corresponds exactly to the number of side-by-side words (tokens) each kernel will take into account ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 150)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tk.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "model_cnn = Sequential()\n",
    "\n",
    "model_cnn.add(layers.Embedding(input_dim=vocab_size+1,\n",
    "                           input_length=150,\n",
    "                           output_dim=50,\n",
    "                           mask_zero=True))\n",
    "model_cnn.add(layers.Conv1D(30,kernel_size=4,activation='tanh'))\n",
    "model_cnn.add(layers.Flatten())\n",
    "model_cnn.add(layers.Dense(5,activation='relu'))\n",
    "model_cnn.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model_cnn.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Look at the number of parameters. You can compare it to the model that you had in previous exercises (esp. the first one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 150, 50)           1521000   \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 147, 30)           6030      \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 4410)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 22055     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,549,091\n",
      "Trainable params: 1,549,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Fit your model with a stopping criterion, and evaluate it on the test data.\n",
    "\n",
    "You will probably notice that it is ... **much faster** than RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6918 - accuracy: 0.5257 - val_loss: 0.6862 - val_accuracy: 0.5467\n",
      "Epoch 2/20\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5372 - accuracy: 0.8394 - val_loss: 0.5178 - val_accuracy: 0.7627\n",
      "Epoch 3/20\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2066 - accuracy: 0.9474 - val_loss: 0.4755 - val_accuracy: 0.7680\n",
      "Epoch 4/20\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 0.9914 - val_loss: 0.5088 - val_accuracy: 0.7867\n",
      "Epoch 5/20\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9971 - val_loss: 0.4774 - val_accuracy: 0.8147\n",
      "Epoch 6/20\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5021 - val_accuracy: 0.8053\n",
      "Epoch 7/20\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 3.0092e-04 - accuracy: 1.0000 - val_loss: 0.5803 - val_accuracy: 0.8133\n",
      "Epoch 8/20\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 1.3452e-04 - accuracy: 1.0000 - val_loss: 0.6864 - val_accuracy: 0.8160\n",
      "The accuracy evaluated on the test set is of 77.080%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model_cnn.fit(X_train_pad, y_train, \n",
    "          epochs=20, \n",
    "          batch_size=32,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",
    "\n",
    "\n",
    "res = model_cnn.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Learn a Word2Vec representation, and then feed it into a NN with a `Conv1D`üî• "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the first part of the exercise, you were asked to jointly learn the embedding representation and the CNN convolution within the same network, which was the CNN counterpart of the left part of this Figure: \n",
    "\n",
    "<img src=\"embedding_or_RNN.png\" width=\"700px\" />\n",
    "\n",
    "Now, let's try to replace the RNN with a CNN for an architecture, as shown on the right side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question** ‚ùì Learn a Word2Vec model or load a trained one directly from GENSIM (transfer learning). Then, prepare your data as in the previous exercise. This question is quite long but it prepares you for real=world challenges. Don't worry, you have already done all the building bricks in the previous exercises!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "word2vec = Word2Vec(sentences=X_train,vector_size=150,min_count=4,window=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32',maxlen=150)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32',maxlen=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Now, build a model that has a `Conv1D` layer, a `Flatten` layer, a `Dense` layer, and an `output` layer. Compile and fit it on the train data. You can then evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(64,kernel_size=4,activation='tanh'))\n",
    "model.add(layers.Conv1D(128,kernel_size=4,activation='tanh'))\n",
    "model.add(layers.Conv1D(256,kernel_size=4,activation='tanh'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì You might be frustrated by the accuracy you got, this happens to us all at some point. Once you have tested your first iteration, you need to improve your models: by making them more complex, changing the parameters, stacking additional layers, and so on...\n",
    "\n",
    "Only practice and experimentation will get you there. So you can go back to your previous models, change them and try to get better results ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "105/105 [==============================] - 3s 26ms/step - loss: 1.5075 - accuracy: 0.5197 - val_loss: 0.7186 - val_accuracy: 0.4709\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 3s 26ms/step - loss: 1.3650 - accuracy: 0.5550 - val_loss: 0.6979 - val_accuracy: 0.5630\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 3s 26ms/step - loss: 0.7050 - accuracy: 0.6099 - val_loss: 0.8286 - val_accuracy: 0.5242\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 3s 26ms/step - loss: 0.5899 - accuracy: 0.6941 - val_loss: 0.7639 - val_accuracy: 0.5484\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 3s 26ms/step - loss: 0.4504 - accuracy: 0.7879 - val_loss: 1.2373 - val_accuracy: 0.5206\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 3s 26ms/step - loss: 0.3118 - accuracy: 0.8590 - val_loss: 1.0373 - val_accuracy: 0.5400\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 3s 26ms/step - loss: 0.1966 - accuracy: 0.9229 - val_loss: 1.5938 - val_accuracy: 0.5448\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "history = model.fit(X_train_pad,y_train,epochs=100,validation_split=0.33,batch_size=16,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluated on the test set is of 55.120%\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations! Natural Language Processing is a complex topic and usually only a few students reach this challenge. You can be proud of yourself!\n",
    "\n",
    "üê£ If you are reading this notebook after the bootcamp, it is also totally normal as there are plenty of concepts to learn and digest!\n",
    "\n",
    "üíæ Don't forget to `git add / commit / push`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
